{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wRam3IzGD-cT",
        "outputId": "6bcaa7fb-42f1-4990-93fe-560584f8c1d8"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/DeepQuestAI/Fire-Smoke-Dataset/releases/download/v1/FIRE-SMOKE-DATASET.zip\n",
        "!unzip FIRE-SMOKE-DATASET.zip\n",
        "\n",
        "# Removing train and test data for \"Smoke\" images\n",
        "import shutil\n",
        "shutil.rmtree('FIRE-SMOKE-DATASET/Test/Smoke')\n",
        "shutil.rmtree('FIRE-SMOKE-DATASET/Train/Smoke')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLHztTF7EWDz"
      },
      "outputs": [],
      "source": [
        "%pip install keras_preprocessing keras tensorflow scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0_kP1DPBEgFF",
        "outputId": "51942fed-e6f6-46af-f4c5-08b35eb4b132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1800 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"FIRE-SMOKE-DATASET/Train\"\n",
        "\n",
        "training_datagen = ImageDataGenerator(rescale=1./255,zoom_range=0.15,horizontal_flip=True,fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"FIRE-SMOKE-DATASET/Test\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(224,224),\n",
        "\tshuffle = True,\n",
        "\tclass_mode='categorical',\n",
        "  batch_size = 128\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(224,224),\n",
        "\tclass_mode='categorical',\n",
        "\tshuffle = True,\n",
        "  batch_size= 14\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R2XCe-suGToa",
        "outputId": "dfbd3547-0bc3-4413-e699-27bb5da42723"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-18 08:13:13.002520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-05-18 08:13:13.002770: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.002831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.002879: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.002926: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.002974: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.003019: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.003064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.003127: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2023-05-18 08:13:13.003139: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2023-05-18 08:13:13.003433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
        "\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "\n",
        "base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(2048, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "M_PeHnQ4GkeK",
        "outputId": "3cd87653-29df-4924-a5d5-8edf1439ebae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-18 08:13:25.070906: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 389316608 exceeds 10% of free system memory.\n",
            "2023-05-18 08:13:25.516451: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 265814016 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1/14 [=>............................] - ETA: 2:20 - loss: 0.7114 - acc: 0.5859"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-18 08:13:29.200100: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 389316608 exceeds 10% of free system memory.\n",
            "2023-05-18 08:13:29.633197: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 265814016 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2/14 [===>..........................] - ETA: 49s - loss: 0.6571 - acc: 0.7305 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-18 08:13:33.314366: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 389316608 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 87s 6s/step - loss: 7.3443 - acc: 0.8131 - val_loss: 0.1318 - val_acc: 0.9490\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 84s 6s/step - loss: 0.2606 - acc: 0.9187 - val_loss: 0.0977 - val_acc: 0.9643\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1975 - acc: 0.9258 - val_loss: 0.1007 - val_acc: 0.9592\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 83s 6s/step - loss: 0.1244 - acc: 0.9522 - val_loss: 0.2947 - val_acc: 0.8673\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 82s 6s/step - loss: 0.1478 - acc: 0.9480 - val_loss: 0.4111 - val_acc: 0.8418\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 84s 6s/step - loss: 0.2301 - acc: 0.9462 - val_loss: 0.2962 - val_acc: 0.8929\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 97s 7s/step - loss: 0.0816 - acc: 0.9713 - val_loss: 0.2162 - val_acc: 0.9388\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 97s 7s/step - loss: 0.0679 - acc: 0.9767 - val_loss: 0.2726 - val_acc: 0.8980\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 97s 7s/step - loss: 0.0514 - acc: 0.9797 - val_loss: 0.1214 - val_acc: 0.9745\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 96s 7s/step - loss: 0.2820 - acc: 0.9133 - val_loss: 0.1758 - val_acc: 0.9439\n"
          ]
        }
      ],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('val_loss')<=0.1099 and logs.get('loss')<=0.1099):\n",
        "      print('\\n\\n Reached The Destination!')\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 14,\n",
        "    epochs = 10,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = 14,\n",
        "    callbacks=[callbacks]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4H7HFmh2GmUk",
        "outputId": "69accfd5-7444-461b-d280-de48820c317a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 131s 9s/step - loss: 0.1344 - acc: 0.9653 - val_loss: 0.4466 - val_acc: 0.8827\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 119s 8s/step - loss: 0.0230 - acc: 0.9934 - val_loss: 0.3666 - val_acc: 0.9388\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 119s 8s/step - loss: 0.0648 - acc: 0.9761 - val_loss: 0.3040 - val_acc: 0.9490\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 122s 9s/step - loss: 0.0297 - acc: 0.9910 - val_loss: 0.1540 - val_acc: 0.9745\n",
            "Epoch 5/10\n",
            " 8/14 [================>.............] - ETA: 54s - loss: 0.0152 - acc: 0.9941 "
          ]
        }
      ],
      "source": [
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('val_loss')<=0.1099 and logs.get('loss')<=0.1099):\n",
        "      print('\\n\\n Reached The Destination!')\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 14,\n",
        "    epochs = 10,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = 14,\n",
        "    callbacks=[callbacks]\n",
        ")\n",
        "print(len(base_model.layers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('model_file.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPp9esGG/8oy8x49Nu/XcE1",
      "include_colab_link": true,
      "name": "Project-Fire Detection System.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
